<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="前言
前排提示:ollama很吃性能的，别拿你的生产力的小鸡跑
建议内存大点的鸡上跑
1 docker安装openwebui&#43;ollama
https://github.com/open-webui/open-webui
For CPU Only: If you&rsquo;re not using a GPU, use this command instead:">  

  <title>
    
      拿vps跑deepseek r1
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.65ae318a1ec6e7795879f61af92fe7a407a4b2e59adeeb02b3892f4bf792ef7945c69626b1d0e0960725e3173c7d988b1ca729bac9d9c4b3e2117f20e01be3e0.css" integrity="sha512-Za4xih7G53lYefYa&#43;S/npAeksuWa3usCs4kvS/eS73lFxpYmsdDglgcl4xc8fZiLHKcpusnZxLPiEX8g4Bvj4A==" />
  
</head>
<body a="dark">
        <main class="page-content" aria-label="Content">
            <div class="w">
                <div class="post-meta">
                    <a href="/">..</a>

                    <p>
                        <time datetime="2025-01-29 17:27:13 &#43;0000 UTC">
                            2025-01-29
                        </time>
                    </p>
                </div>

<article>
    <h1>拿vps跑deepseek r1</h1>

    

    <h1 id="前言">前言</h1>
<p>前排提示:ollama很吃性能的，别拿你的生产力的小鸡跑<br>
建议内存大点的鸡上跑</p>
<h1 id="1-docker安装openwebuiollama">1 docker安装openwebui+ollama</h1>
<p><a href="https://github.com/open-webui/open-webui">https://github.com/open-webui/open-webui</a><br>
For CPU Only: If you&rsquo;re not using a GPU, use this command instead:</p>
<pre tabindex="0"><code>docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
</code></pre><h1 id="2-访问及设置">2 访问及设置</h1>
<h2 id="-访问">① 访问</h2>
<p>ip:3000端口打开即可，有条件的话nginx反代下</p>
<h2 id="-设置用户名密码邮箱">② 设置用户名密码邮箱</h2>
<p><img src="https://s3.qklg.net/img/YYnrsjR.png" alt="image-20250129170712679"></p>
<p>##③ 切换语言</p>
<p>点击右上角头像下面的settings-general，选择中文，save保存</p>
<p><img src="https://s3.qklg.net/img/0GbOotp.png" alt="image-20250129170809543"></p>
<p><img src="https://s3.qklg.net/img/0fPh5qs.png" alt="image-20250129170824068"></p>
<h2 id="-添加模型">④ 添加模型</h2>
<p>左上角选择一个模型的话可以输入你想要的模型，</p>
<p>我们选择7b的，输入 deepseek-r1:7b,从ollama拉取</p>
<p><img src="https://s3.qklg.net/img/nOPOWHY.png" alt="image-20250129171023372"></p>
<p>下载deepseek的7b模型，小鸡的性能跑个7b还是可以的<br>
如果没法跑的话，可以跑1.5b的  deepseek-r1:1.5b</p>
<p>其他的模型的话这边<br>
<a href="https://ollama.com/library/deepseek-r1">https://ollama.com/library/deepseek-r1</a></p>
<h1 id="3-闲言碎语">3 闲言碎语</h1>
<p>本人用签名探针上的家里云。配置为5600+32G内存跑的</p>
<p>占用的话大概cpu在50%左右，内存吃到10G<br>
我的cpu的话跑分gb5单核1675多，多核8934，你可以参考下自己跑着玩</p>
<p><img src="https://s3.qklg.net/img/cpzRGgs.png" alt="image-20241110200910735"></p>
<p>7.5b的效果其实不怎么样，你们跑了就知道了</p>
<p><img src="https://s3.qklg.net/img/IsaW10B.png" alt=""></p>
<p>其实还不如自己直接调用api来的合适</p>
<p>跑api的话可以参考我这个帖子</p>
<p><a href="https://qklg.net/post/20250127/">https://qklg.net/post/20250127/</a></p>
<p>有条件上显卡跑，而且显存至少16G的，这种纯cpu跑的是玩具<br>
隔壁老哥拿12400跑72b的https://linux.do/t/topic/397701/20<br>
半小时才能回答一次，隔壁老哥cpu当一回时代先锋</p>

</article>

            </div>
        </main>
    </body></html>
