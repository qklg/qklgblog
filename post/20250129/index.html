<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>拿vps跑deepseek r1 | qklg</title>
<meta name=keywords content><meta name=description content="前言 前排提示:ollama很吃性能的，别拿你的生产力的小鸡跑
建议内存大点的鸡上跑
1 docker安装openwebui+ollama https://github.com/open-webui/open-webui
For CPU Only: If you&rsquo;re not using a GPU, use this command instead:
docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama 2 访问及设置 ① 访问 ip:3000端口打开即可，有条件的话nginx反代下
② 设置用户名密码邮箱 ##③ 切换语言
点击右上角头像下面的settings-general，选择中文，save保存
④ 添加模型 左上角选择一个模型的话可以输入你想要的模型，
我们选择7b的，输入 deepseek-r1:7b,从ollama拉取
下载deepseek的7b模型，小鸡的性能跑个7b还是可以的
如果没法跑的话，可以跑1.5b的 deepseek-r1:1.5b
其他的模型的话这边
https://ollama.com/library/deepseek-r1
3 闲言碎语 本人用签名探针上的家里云。配置为5600+32G内存跑的
占用的话大概cpu在50%左右，内存吃到10G
我的cpu的话跑分gb5单核1675多，多核8934，你可以参考下自己跑着玩
7.5b的效果其实不怎么样，你们跑了就知道了
其实还不如自己直接调用api来的合适
跑api的话可以参考我这个帖子
https://qklg.net/post/20250127/
有条件上显卡跑，而且显存至少16G的，这种纯cpu跑的是玩具
隔壁老哥拿12400跑72b的https://linux.do/t/topic/397701/20
半小时才能回答一次，隔壁老哥cpu当一回时代先锋"><meta name=author content="Me"><link rel=canonical href=https://blog.qklg.net/post/20250129/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.qklg.net/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://blog.qklg.net/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://blog.qklg.net/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://blog.qklg.net/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://blog.qklg.net/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.qklg.net/post/20250129/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="拿vps跑deepseek r1"><meta property="og:description" content="前言 前排提示:ollama很吃性能的，别拿你的生产力的小鸡跑
建议内存大点的鸡上跑
1 docker安装openwebui+ollama https://github.com/open-webui/open-webui
For CPU Only: If you&rsquo;re not using a GPU, use this command instead:
docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama 2 访问及设置 ① 访问 ip:3000端口打开即可，有条件的话nginx反代下
② 设置用户名密码邮箱 ##③ 切换语言
点击右上角头像下面的settings-general，选择中文，save保存
④ 添加模型 左上角选择一个模型的话可以输入你想要的模型，
我们选择7b的，输入 deepseek-r1:7b,从ollama拉取
下载deepseek的7b模型，小鸡的性能跑个7b还是可以的
如果没法跑的话，可以跑1.5b的 deepseek-r1:1.5b
其他的模型的话这边
https://ollama.com/library/deepseek-r1
3 闲言碎语 本人用签名探针上的家里云。配置为5600+32G内存跑的
占用的话大概cpu在50%左右，内存吃到10G
我的cpu的话跑分gb5单核1675多，多核8934，你可以参考下自己跑着玩
7.5b的效果其实不怎么样，你们跑了就知道了
其实还不如自己直接调用api来的合适
跑api的话可以参考我这个帖子
https://qklg.net/post/20250127/
有条件上显卡跑，而且显存至少16G的，这种纯cpu跑的是玩具
隔壁老哥拿12400跑72b的https://linux.do/t/topic/397701/20
半小时才能回答一次，隔壁老哥cpu当一回时代先锋"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.qklg.net/post/20250129/"><meta property="og:image" content="https://blog.qklg.net/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-29T17:27:13+00:00"><meta property="article:modified_time" content="2025-01-29T17:27:13+00:00"><meta property="og:site_name" content="qklg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blog.qklg.net/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="拿vps跑deepseek r1"><meta name=twitter:description content="前言 前排提示:ollama很吃性能的，别拿你的生产力的小鸡跑
建议内存大点的鸡上跑
1 docker安装openwebui+ollama https://github.com/open-webui/open-webui
For CPU Only: If you&rsquo;re not using a GPU, use this command instead:
docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama 2 访问及设置 ① 访问 ip:3000端口打开即可，有条件的话nginx反代下
② 设置用户名密码邮箱 ##③ 切换语言
点击右上角头像下面的settings-general，选择中文，save保存
④ 添加模型 左上角选择一个模型的话可以输入你想要的模型，
我们选择7b的，输入 deepseek-r1:7b,从ollama拉取
下载deepseek的7b模型，小鸡的性能跑个7b还是可以的
如果没法跑的话，可以跑1.5b的 deepseek-r1:1.5b
其他的模型的话这边
https://ollama.com/library/deepseek-r1
3 闲言碎语 本人用签名探针上的家里云。配置为5600+32G内存跑的
占用的话大概cpu在50%左右，内存吃到10G
我的cpu的话跑分gb5单核1675多，多核8934，你可以参考下自己跑着玩
7.5b的效果其实不怎么样，你们跑了就知道了
其实还不如自己直接调用api来的合适
跑api的话可以参考我这个帖子
https://qklg.net/post/20250127/
有条件上显卡跑，而且显存至少16G的，这种纯cpu跑的是玩具
隔壁老哥拿12400跑72b的https://linux.do/t/topic/397701/20
半小时才能回答一次，隔壁老哥cpu当一回时代先锋"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.qklg.net/posts/"},{"@type":"ListItem","position":2,"name":"拿vps跑deepseek r1","item":"https://blog.qklg.net/post/20250129/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"拿vps跑deepseek r1","name":"拿vps跑deepseek r1","description":"前言 前排提示:ollama很吃性能的，别拿你的生产力的小鸡跑\n建议内存大点的鸡上跑\n1 docker安装openwebui+ollama https://github.com/open-webui/open-webui\nFor CPU Only: If you\u0026rsquo;re not using a GPU, use this command instead:\ndocker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama 2 访问及设置 ① 访问 ip:3000端口打开即可，有条件的话nginx反代下\n② 设置用户名密码邮箱 ##③ 切换语言\n点击右上角头像下面的settings-general，选择中文，save保存\n④ 添加模型 左上角选择一个模型的话可以输入你想要的模型，\n我们选择7b的，输入 deepseek-r1:7b,从ollama拉取\n下载deepseek的7b模型，小鸡的性能跑个7b还是可以的\n如果没法跑的话，可以跑1.5b的 deepseek-r1:1.5b\n其他的模型的话这边\nhttps://ollama.com/library/deepseek-r1\n3 闲言碎语 本人用签名探针上的家里云。配置为5600+32G内存跑的\n占用的话大概cpu在50%左右，内存吃到10G\n我的cpu的话跑分gb5单核1675多，多核8934，你可以参考下自己跑着玩\n7.5b的效果其实不怎么样，你们跑了就知道了\n其实还不如自己直接调用api来的合适\n跑api的话可以参考我这个帖子\nhttps://qklg.net/post/20250127/\n有条件上显卡跑，而且显存至少16G的，这种纯cpu跑的是玩具\n隔壁老哥拿12400跑72b的https://linux.do/t/topic/397701/20\n半小时才能回答一次，隔壁老哥cpu当一回时代先锋","keywords":[],"articleBody":"前言 前排提示:ollama很吃性能的，别拿你的生产力的小鸡跑\n建议内存大点的鸡上跑\n1 docker安装openwebui+ollama https://github.com/open-webui/open-webui\nFor CPU Only: If you’re not using a GPU, use this command instead:\ndocker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama 2 访问及设置 ① 访问 ip:3000端口打开即可，有条件的话nginx反代下\n② 设置用户名密码邮箱 ##③ 切换语言\n点击右上角头像下面的settings-general，选择中文，save保存\n④ 添加模型 左上角选择一个模型的话可以输入你想要的模型，\n我们选择7b的，输入 deepseek-r1:7b,从ollama拉取\n下载deepseek的7b模型，小鸡的性能跑个7b还是可以的\n如果没法跑的话，可以跑1.5b的 deepseek-r1:1.5b\n其他的模型的话这边\nhttps://ollama.com/library/deepseek-r1\n3 闲言碎语 本人用签名探针上的家里云。配置为5600+32G内存跑的\n占用的话大概cpu在50%左右，内存吃到10G\n我的cpu的话跑分gb5单核1675多，多核8934，你可以参考下自己跑着玩\n7.5b的效果其实不怎么样，你们跑了就知道了\n其实还不如自己直接调用api来的合适\n跑api的话可以参考我这个帖子\nhttps://qklg.net/post/20250127/\n有条件上显卡跑，而且显存至少16G的，这种纯cpu跑的是玩具\n隔壁老哥拿12400跑72b的https://linux.do/t/topic/397701/20\n半小时才能回答一次，隔壁老哥cpu当一回时代先锋\n","wordCount":"65","inLanguage":"en","image":"https://blog.qklg.net/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-01-29T17:27:13Z","dateModified":"2025-01-29T17:27:13Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.qklg.net/post/20250129/"},"publisher":{"@type":"Organization","name":"qklg","logo":{"@type":"ImageObject","url":"https://blog.qklg.net/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.qklg.net/ accesskey=h title="qklg (Alt + H)"><img src=https://blog.qklg.net/apple-touch-icon.png alt aria-label=logo height=35>qklg</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.qklg.net/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://blog.qklg.net/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://blog.qklg.net/about/ title=About><span>About</span></a></li><li><a href=https://blog.qklg.net/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://blog.qklg.net/index.xml title=RSS><span>RSS</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.qklg.net/>Home</a>&nbsp;»&nbsp;<a href=https://blog.qklg.net/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">拿vps跑deepseek r1</h1><div class=post-meta><span title='2025-01-29 17:27:13 +0000 UTC'>2025-01-29</span>&nbsp;·&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#-访问>① 访问</a></li><li><a href=#-设置用户名密码邮箱>② 设置用户名密码邮箱</a></li><li><a href=#-添加模型>④ 添加模型</a></li></ul></nav></div></details></div><div class=post-content><h1 id=前言>前言<a hidden class=anchor aria-hidden=true href=#前言>#</a></h1><p>前排提示:ollama很吃性能的，别拿你的生产力的小鸡跑<br>建议内存大点的鸡上跑</p><h1 id=1-docker安装openwebuiollama>1 docker安装openwebui+ollama<a hidden class=anchor aria-hidden=true href=#1-docker安装openwebuiollama>#</a></h1><p><a href=https://github.com/open-webui/open-webui>https://github.com/open-webui/open-webui</a><br>For CPU Only: If you&rsquo;re not using a GPU, use this command instead:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
</span></span></code></pre></div><h1 id=2-访问及设置>2 访问及设置<a hidden class=anchor aria-hidden=true href=#2-访问及设置>#</a></h1><h2 id=-访问>① 访问<a hidden class=anchor aria-hidden=true href=#-访问>#</a></h2><p>ip:3000端口打开即可，有条件的话nginx反代下</p><h2 id=-设置用户名密码邮箱>② 设置用户名密码邮箱<a hidden class=anchor aria-hidden=true href=#-设置用户名密码邮箱>#</a></h2><p><img loading=lazy src=https://s3.qklg.net/img/YYnrsjR.png alt=image-20250129170712679></p><p>##③ 切换语言</p><p>点击右上角头像下面的settings-general，选择中文，save保存</p><p><img loading=lazy src=https://s3.qklg.net/img/0GbOotp.png alt=image-20250129170809543></p><p><img loading=lazy src=https://s3.qklg.net/img/0fPh5qs.png alt=image-20250129170824068></p><h2 id=-添加模型>④ 添加模型<a hidden class=anchor aria-hidden=true href=#-添加模型>#</a></h2><p>左上角选择一个模型的话可以输入你想要的模型，</p><p>我们选择7b的，输入 deepseek-r1:7b,从ollama拉取</p><p><img loading=lazy src=https://s3.qklg.net/img/nOPOWHY.png alt=image-20250129171023372></p><p>下载deepseek的7b模型，小鸡的性能跑个7b还是可以的<br>如果没法跑的话，可以跑1.5b的 deepseek-r1:1.5b</p><p>其他的模型的话这边<br><a href=https://ollama.com/library/deepseek-r1>https://ollama.com/library/deepseek-r1</a></p><h1 id=3-闲言碎语>3 闲言碎语<a hidden class=anchor aria-hidden=true href=#3-闲言碎语>#</a></h1><p>本人用签名探针上的家里云。配置为5600+32G内存跑的</p><p>占用的话大概cpu在50%左右，内存吃到10G<br>我的cpu的话跑分gb5单核1675多，多核8934，你可以参考下自己跑着玩</p><p><img loading=lazy src=https://s3.qklg.net/img/cpzRGgs.png alt=image-20241110200910735></p><p>7.5b的效果其实不怎么样，你们跑了就知道了</p><p><img loading=lazy src=https://s3.qklg.net/img/IsaW10B.png alt></p><p>其实还不如自己直接调用api来的合适</p><p>跑api的话可以参考我这个帖子</p><p><a href=https://qklg.net/post/20250127/>https://qklg.net/post/20250127/</a></p><p>有条件上显卡跑，而且显存至少16G的，这种纯cpu跑的是玩具<br>隔壁老哥拿12400跑72b的https://linux.do/t/topic/397701/20<br>半小时才能回答一次，隔壁老哥cpu当一回时代先锋</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://blog.qklg.net/post/20250205/><span class=title>« Prev</span><br><span>自用脚本备份</span>
</a><a class=next href=https://blog.qklg.net/post/20250127/><span class=title>Next »</span><br><span>用nextchat调用deepseek的api</span></a></nav></footer></article></main><footer class=footer><span>qklg Hosted on <a href=https://pages.cloudflare.com/ target=_blank>Cloudflare Pages </a></span>·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>